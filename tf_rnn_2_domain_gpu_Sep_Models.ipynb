{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLWwKqLHCBBK"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.losses as kls\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "gpu_num = 0\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[gpu_num], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9v15E_3P8gR"
   },
   "outputs": [],
   "source": [
    "main_folder_path = Path('.')\n",
    "# folder_path = main_folder_path / 'Smaller_data3'\n",
    "disp = False\n",
    "\n",
    "class DataGen:\n",
    "    def __init__(self, batch_size=32, dim_x = 2, dim_y = 1):\n",
    "\n",
    "        self.bs = batch_size\n",
    "        self.main_folder_path = Path('.')\n",
    "        self.folder_path = main_folder_path / 'Smaller_data3'\n",
    "\n",
    "        self.dim_x = dim_x\n",
    "        self.dim_y = dim_y\n",
    "        self.B_max = 0.0\n",
    "        # self.B_min = 0.0\n",
    "        self.seq_len = {}\n",
    "\n",
    "        self.df_seq_len = pd.DataFrame(columns = ['FileName',\n",
    "                                                  'SeqLength_domain',\n",
    "                                                  'SeqLength_coil',\n",
    "                                                  'SeqLength_interface'])\n",
    "\n",
    "        self.get_sequence_len()\n",
    "        self.df_seq_len.sort_values(by='SeqLength_domain', ascending=False, inplace=True)\n",
    "\n",
    "        self.data_size = self.df_seq_len.shape[0]\n",
    "        self.threshold_points = 300\n",
    "        # self.norm_denom = self.B_max - self.B_min\n",
    "\n",
    "        # self.max_seq_len_global = self.df_seq_len.SeqLength.max()\n",
    "\n",
    "\n",
    "    def get_sequence_len(self):\n",
    "\n",
    "        files = glob.glob1(self.folder_path, \"*.csv\")\n",
    "\n",
    "        for csv_file in files:\n",
    "            file_path = self.folder_path / csv_file\n",
    "            df = pd.read_csv(file_path.absolute(), header=None)\n",
    "\n",
    "            if self.B_max < df[2].max():\n",
    "                self.B_max = df[2].max()\n",
    "\n",
    "            counts = df[3].value_counts()\n",
    "\n",
    "            # 2.0    1904\n",
    "            # 0.0     248\n",
    "            # 1.0     113\n",
    "            # Name: 3, dtype: int64\n",
    "\n",
    "            record = {'FileName': csv_file,\n",
    "                      'SeqLength_domain': counts[2],\n",
    "                      'SeqLength_coil': counts[0],\n",
    "                      'SeqLength_interface': counts[1]}\n",
    "\n",
    "            self.df_seq_len = self.df_seq_len.append(record, ignore_index = True)\n",
    "\n",
    "    def data_feed(self, index):\n",
    "\n",
    "        if (index + self.bs) >= self.df_seq_len.shape[0]:\n",
    "            index = self.df_seq_len.shape[0] - self.bs - 1\n",
    "\n",
    "        # batch_files = list(self.seq_len_sorted)[index : index + self.bs]\n",
    "        batch_files = self.df_seq_len['FileName'].iloc[index : index + self.bs].tolist()\n",
    "\n",
    "        max_seq_len_domain = max(self.df_seq_len['SeqLength_domain'].iloc[index : index + self.bs].tolist())\n",
    "        if max_seq_len_domain > self.threshold_points:\n",
    "            max_seq_len_domain = self.threshold_points\n",
    "\n",
    "        max_seq_len_coil = max(self.df_seq_len['SeqLength_coil'].iloc[index : index + self.bs].tolist())\n",
    "        if max_seq_len_coil > self.threshold_points:\n",
    "            max_seq_len_coil = self.threshold_points\n",
    "\n",
    "        max_seq_len_interface = max(self.df_seq_len['SeqLength_interface'].iloc[index : index + self.bs].tolist())\n",
    "        if max_seq_len_interface > self.threshold_points:\n",
    "            max_seq_len_interface = self.threshold_points\n",
    "\n",
    "        max_seq_len_domain += max_seq_len_interface\n",
    "        max_seq_len_coil += max_seq_len_interface\n",
    "\n",
    "        src_len = []\n",
    "\n",
    "        X_domain = np.zeros([self.bs, max_seq_len_domain, self.dim_x])\n",
    "        X_coil = np.zeros([self.bs, max_seq_len_coil, self.dim_x])\n",
    "        # X_interface = np.zeros([self.bs, max_seq_len_interface, self.dim_x])\n",
    "\n",
    "        y_domain = np.zeros([self.bs, max_seq_len_domain])\n",
    "        y_coil = np.zeros([self.bs, max_seq_len_coil])\n",
    "        # y_interface = np.zeros([self.bs, max_seq_len_interface])\n",
    "\n",
    "        mask_domain = np.ones_like(X_domain)\n",
    "        mask_coil = np.ones_like(X_coil)\n",
    "        # mask_interface = np.ones_like(X_interface)\n",
    "\n",
    "        # print(X.shape, y.shape)\n",
    "\n",
    "        for inx, csv_file in enumerate(batch_files):\n",
    "\n",
    "            file_path = self.folder_path / csv_file\n",
    "\n",
    "            df = pd.read_csv(file_path.absolute(), header=None)\n",
    "            df.rename(columns={0: 'x', 1: 'y', 2: 'B', 3: 'mat'}, inplace=True)\n",
    "\n",
    "            ### Domain\n",
    "            seq_len_domain = self.df_seq_len[self.df_seq_len['FileName'] == csv_file]['SeqLength_domain'].iloc[0]\n",
    "            if seq_len_domain > self.threshold_points:\n",
    "                seq_len_domain = self.threshold_points\n",
    "\n",
    "                df_X = df.loc[df['mat'] == 2, ['x', 'y']]\n",
    "                sample_inx = np.random.randint(0, df_X.shape[0], self.threshold_points)\n",
    "                X_domain[inx, :seq_len_domain, :] = df_X.iloc[sample_inx, :].values\n",
    "\n",
    "                df_y = df.loc[df['mat'] == 2, ['B']]\n",
    "                y_domain[inx, :seq_len_domain] = df_y.iloc[sample_inx, :].values.squeeze()\n",
    "\n",
    "                mask_domain[inx, seq_len_domain:, :] = 0\n",
    "\n",
    "            else:\n",
    "                X_domain[inx, :seq_len_domain, :] = df.loc[df['mat'] == 2, ['x', 'y']].values\n",
    "                y_domain[inx, :seq_len_domain] = df.loc[df['mat'] == 2, ['B']].values.squeeze()\n",
    "#                 y_domain /= self.B_max\n",
    "                mask_domain[inx, seq_len_domain:, :] = 0\n",
    "\n",
    "            ### Coil\n",
    "            seq_len_coil = self.df_seq_len[self.df_seq_len['FileName'] == csv_file]['SeqLength_coil'].iloc[0]\n",
    "            if seq_len_coil > self.threshold_points:\n",
    "                seq_len_coil = self.threshold_points\n",
    "\n",
    "                df_X = df.loc[df['mat'] == 0, ['x', 'y']]\n",
    "                sample_inx = np.random.randint(0, df_X.shape[0], self.threshold_points)\n",
    "                X_coil[inx, :seq_len_coil, :] = df_X.iloc[sample_inx, :].values\n",
    "\n",
    "                df_y = df.loc[df['mat'] == 0, ['B']]\n",
    "                y_coil[inx, :seq_len_coil] = df_y.iloc[sample_inx, :].values.squeeze()\n",
    "                mask_coil[inx, seq_len_coil:, :] = 0\n",
    "\n",
    "            else:\n",
    "                X_coil[inx, :seq_len_coil, :] = df.loc[df['mat'] == 0, ['x', 'y']].values\n",
    "                y_coil[inx, :seq_len_coil] = df.loc[df['mat'] == 0, ['B']].values.squeeze()\n",
    "#                 y_coil /= self.B_max\n",
    "                mask_coil[inx, seq_len_coil:, :] = 0\n",
    "\n",
    "\n",
    "            ## Interface - new\n",
    "            seq_len_interface = self.df_seq_len[self.df_seq_len['FileName'] == csv_file]['SeqLength_interface'].iloc[0]\n",
    "            df_X = df.loc[df['mat'] == 1, ['x', 'y']]\n",
    "            df_y = df.loc[df['mat'] == 1, ['B']]\n",
    "\n",
    "            if seq_len_interface > self.threshold_points:\n",
    "                seq_len_interface = self.threshold_points\n",
    "                sample_inx = np.random.randint(0, df_X.shape[0], self.threshold_points)\n",
    "                X_int = df_X.iloc[sample_inx, :].values\n",
    "                y_int = df_y.iloc[sample_inx, :].values.squeeze()\n",
    "            else:\n",
    "                X_int = df.loc[df['mat'] == 1, ['x', 'y']].values\n",
    "                y_int = df.loc[df['mat'] == 1, ['B']].values.squeeze()\n",
    "\n",
    "            # Int - domain\n",
    "            inx_start = seq_len_domain\n",
    "            inx_stop = seq_len_domain + seq_len_interface\n",
    "\n",
    "            X_domain[inx, inx_start:inx_stop, :] = X_int\n",
    "            y_domain[inx, inx_start:inx_stop] = y_int.squeeze()\n",
    "            y_domain[inx, :] /= self.B_max\n",
    "            mask_domain[inx, inx_start:inx_stop, :] = 1\n",
    "\n",
    "            # Int - coil\n",
    "            inx_start = seq_len_coil\n",
    "            inx_stop = seq_len_coil + seq_len_interface\n",
    "\n",
    "            X_coil[inx, inx_start:inx_stop, :] = X_int\n",
    "            y_coil[inx, inx_start:inx_stop] =  y_int\n",
    "            y_coil[inx, :] /= self.B_max\n",
    "            mask_coil[inx, inx_start:inx_stop, :] = 1\n",
    "\n",
    "            ## Interface - old\n",
    "            # seq_len_interface = self.df_seq_len[self.df_seq_len['FileName'] == csv_file]['SeqLength_interface'].iloc[0]\n",
    "            # if seq_len_interface > self.threshold_points:\n",
    "            #   seq_len_interface = self.threshold_points\n",
    "\n",
    "            #   df_X = df.loc[df['mat'] == 1, ['x', 'y']]\n",
    "            #   sample_inx = np.random.randint(0, df_X.shape[0], self.threshold_points)\n",
    "            #   X_interface[inx, :seq_len_interface, :] = df_X.iloc[sample_inx, :].values\n",
    "\n",
    "            #   df_y = df.loc[df['mat'] == 1, ['B']]\n",
    "            #   y_interface[inx, :seq_len_interface] = df_y.iloc[sample_inx, :].values.squeeze()\n",
    "\n",
    "            #   mask_interface[inx, seq_len_interface:, :] = 0\n",
    "\n",
    "            # else:\n",
    "            #   X_interface[inx, :seq_len_interface, :] = df.loc[df['mat'] == 1, ['x', 'y']].values\n",
    "            #   y_interface[inx, :seq_len_interface] = df.loc[df['mat'] == 1, ['B']].values.squeeze()\n",
    "            #   # y_interface /= self.B_max\n",
    "            #   mask_interface[inx, seq_len_interface:, :] = 0\n",
    "\n",
    "            # src_len.append((seq_len_domain, seq_len_coil, seq_len_interface))\n",
    "\n",
    "            # masks = [mask_domain, mask_coil, mask_interface]\n",
    "            # X = [X_domain, X_coil, X_interface]\n",
    "            # y = [y_domain, y_coil, y_interface]\n",
    "\n",
    "            src_len.append((seq_len_domain + seq_len_interface, seq_len_coil + seq_len_interface))\n",
    "\n",
    "            masks = [mask_domain, mask_coil]\n",
    "            X = [X_domain, X_coil]\n",
    "            y = [y_domain, y_coil]\n",
    "\n",
    "        return X, y, masks, src_len\n",
    "\n",
    "    def save_plots(self, x, y, uex, upred, epoch, comment):\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2,figsize=(26,10))\n",
    "\n",
    "        ## True Exact\n",
    "        # ax2.tricontour(x, y, z, levels=20, linewidths=0.5, colors='k')\n",
    "        cntr1 = ax1.tricontourf(x, y, uex, levels=20, cmap=\"jet\")\n",
    "\n",
    "        fig.colorbar(cntr1, ax=ax1)\n",
    "        # ax2.plot(x, y, 'ko', ms=3)\n",
    "        # ax1.set(xlim=(-1, 1), ylim=(-1, 1))\n",
    "        ax1.set_title('Exact')\n",
    "\n",
    "        ## Predictions\n",
    "        # ax2.tricontour(x, y, z, levels=20, linewidths=0.5, colors='k')\n",
    "        cntr2 = ax2.tricontourf(x, y, upred, levels = 20, cmap = 'jet')\n",
    "\n",
    "        fig.colorbar(cntr2, ax=ax2)\n",
    "        # ax2.plot(x, y, 'ko', ms=3)\n",
    "        # ax2.set(xlim=(-1, 1), ylim=(-1, 1))\n",
    "        ax2.set_title('Prediction')\n",
    "\n",
    "        plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "        file_name = f'epoch_{epoch+1}.png'\n",
    "        result_folder = self.main_folder_path / 'Results_Sep_RNN' / comment\n",
    "\n",
    "        if not os.path.exists(result_folder):\n",
    "            os.makedirs(result_folder)\n",
    "\n",
    "        plt.savefig(result_folder.absolute() / file_name, dpi=100)\n",
    "\n",
    "        #plt.savefig(file_name, dpi=100)\n",
    "\n",
    "        #plt.show()\n",
    "        plt.close(fig)\n",
    "        plt.close('all')\n",
    "\n",
    "    def save_stats(self, dict_, epoch, comment):\n",
    "       # with open('history %s' % (epoch), 'w') as outfile:\n",
    "        #    outfile.write(dict_)\n",
    "        result_folder = self.main_folder_path / 'Results_Sep_RNN' / comment\n",
    "\n",
    "        if not os.path.exists(result_folder):\n",
    "            os.makedirs(result_folder)\n",
    "        file_name = f'epoch_{epoch+1}.csv'\n",
    "\n",
    "        print(dict_)\n",
    "        df = pd.DataFrame(dict_, index=[0])\n",
    "        df.to_csv(result_folder.absolute() / file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2hIG6Xt3u1A"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, bi=False):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.bi = bi\n",
    "\n",
    "        if self.bi == True:\n",
    "            self.lstm_1 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
    "                            128, activation='sigmoid',\n",
    "                           dropout= 0.2, recurrent_dropout=0.2,\n",
    "                           kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                            return_sequences= True, return_state= True))\n",
    "\n",
    "            self.BN_1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "            self.lstm_2 = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(\n",
    "                            64, activation='sigmoid',\n",
    "                           dropout= 0.2, recurrent_dropout=0.2,\n",
    "                           kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                            return_sequences= True))\n",
    "\n",
    "            self.BN_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        else:\n",
    "            self.lstm_1 = tf.keras.layers.GRU(\n",
    "                            128, activation='sigmoid',\n",
    "                            dropout= 0.2, recurrent_dropout=0.2,\n",
    "                            kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                            return_sequences= True, return_state= True)\n",
    "\n",
    "            self.BN_1 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "            self.lstm_2 = tf.keras.layers.GRU(\n",
    "                            64, activation='sigmoid',\n",
    "                            dropout= 0.2, recurrent_dropout=0.2,\n",
    "                            kernel_regularizer=l2(0.01), recurrent_regularizer=l2(0.01), bias_regularizer=l2(0.01),\n",
    "                            return_sequences= True)\n",
    "\n",
    "            self.BN_2 = tf.keras.layers.BatchNormalization()\n",
    "\n",
    "        self.dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(1))\n",
    "\n",
    "    def call(self, inputs):\n",
    "\n",
    "        input1 = inputs[0]\n",
    "        mask = inputs[1]\n",
    "\n",
    "#        print(mask.shape)\n",
    "        if self.bi == True:\n",
    "            x, state_h, state_c = self.lstm_1(input1, mask=mask)\n",
    "        else:\n",
    "            x, state_h = self.lstm_1(input1, mask=mask)\n",
    "\n",
    "        x = self.BN_1(x)\n",
    "        x = self.lstm_2(x, mask=mask)\n",
    "        x = self.BN_2(x)\n",
    "        output = self.dense(x, mask=mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETXRJuXf128R"
   },
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B_qQXY3ZbOTj"
   },
   "outputs": [],
   "source": [
    "def my_training(batch, epochs, no_iters, lr, bi, train_summary_writer, comment):\n",
    "\n",
    "    mean_loss = tf.keras.metrics.Mean()\n",
    "    metrics = [tf.keras.metrics.mean_absolute_error]\n",
    "    model = MyModel(bi=bi)\n",
    "    history = {}\n",
    "    optimizer = tf.keras.optimizers.Nadam(learning_rate= lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(no_iters):\n",
    "\n",
    "            index = np.random.randint(batch.data_size-batch.bs)\n",
    "            X_d, X_c, Y_d, Y_c, mask_d, mask_c, seq_len_d, seq_len_c = batch.data_feed(index)\n",
    "\n",
    "            # X, Y, mask, src_len = batch.data_feed(index)\n",
    "#             Y_c *= 1E6\n",
    "            Y_c = np.expand_dims(Y_c, axis=-1)\n",
    "\n",
    "#             Y_d *= 1E6\n",
    "            Y_d = np.expand_dims(Y_d, axis=-1)\n",
    "\n",
    "            # mask = np.logical_not(X) # Finds all the zero values\n",
    "            # returns True (before the NOT) if all the elemenst in the row are zero\n",
    "            # mask = np.logical_not(np.all(mask, axis=-1))\n",
    "\n",
    "            mask_d = np.logical_not(mask_d) # Finds all the zero values\n",
    "            # returns True (before the NOT) if all the elemenst in the row are zero\n",
    "            mask_d = np.logical_not(np.all(mask_d, axis=-1))\n",
    "\n",
    "            mask_c = np.logical_not(mask_c) # Finds all the zero values\n",
    "            # returns True (before the NOT) if all the elemenst in the row are zero\n",
    "            mask_c = np.logical_not(np.all(mask_c, axis=-1))\n",
    "\n",
    "            X_train = [X_d, mask_d]\n",
    "            # print(len(X_train))\n",
    "            # print('X.shape', X_d.shape)\n",
    "            # print('mask.shape', mask_d.shape)\n",
    "            # print('X[:,:, 0].shape', X_d[:,:, 0].shape)\n",
    "            # print('X[:,:, 1].shape',X_d[:,:, 1].shape)\n",
    "            # print('Y.shape', Y_d.shape)\n",
    "            # print('Y_d.max', Y_d.max())\n",
    "            # print('Y_d.min', Y_d.min())\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred = model(X_train, training=True)\n",
    "                # print('y_pred.shape', y_pred.shape)\n",
    "                loss_ = tf.keras.losses.mean_squared_error(Y_d, y_pred)\n",
    "\n",
    "            gradients = tape.gradient(loss_, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            mean_loss(loss_)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print('Epoch: {:02} | Time: {}m {}s\\t'.format(epoch+1, epoch_mins, epoch_secs))\n",
    "        try:\n",
    "            print('step %s: mean loss = %s' % (epoch+1, mean_loss.result().numpy()))\n",
    "            history[epoch] = mean_loss.result().numpy()\n",
    "        except:\n",
    "            print('step %s: mean loss = %s' % (epoch+1, mean_loss.result()))\n",
    "            history[epoch] = mean_loss.result()\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss', mean_loss.result(), step=epoch)\n",
    "\n",
    "        y_pred = model(X_train, training=False)\n",
    "        y_pred = y_pred.numpy()\n",
    "        #print('y_pred.shape', y_pred.shape)\n",
    "        batch.save_plots(X_d[1, :, 0], X_d[1, :, 1], Y_d[1, :,0], y_pred[1, :, 0], epoch, comment)\n",
    "\n",
    "        mean_loss.reset_states()\n",
    "        tf.summary.flush(train_summary_writer)\n",
    "\n",
    "    # Serializing json\n",
    "    #history_object = json.dumps(history, indent = 4)\n",
    "    batch.save_stats(history, epoch, comment)\n",
    "    #print(history_object)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k15E1bKeCKyS"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "bi = False\n",
    "\n",
    "main_folder_path = Path('.')\n",
    "folder_path = main_folder_path / 'Smaller_data3'\n",
    "\n",
    "batch = DataGen(batch_size= batch_size)\n",
    "batch.folder_path = folder_path\n",
    "\n",
    "no_iters = int(batch.df_seq_len.shape[0]/batch_size)*2\n",
    "\n",
    "# index = np.random.randint(batch.df_seq_len.shape[0]-batch.bs)\n",
    "# # X, Y, _, _ = batch.data_feed(index)\n",
    "# X_d, X_c, Y_d, Y_c, mask_d, mask_c, seq_len_d, seq_len_c = batch.data_feed(index)\n",
    "\n",
    "comment = f' App_batch_size={batch_size} lr={lr} Bi-dir={bi}'\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'runs/MagApp/' + current_time + '/train' + comment\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-2CIktJnvLNY"
   },
   "outputs": [],
   "source": [
    "# history = my_training(batch, epochs, no_iters, lr, bi, train_summary_writer, comment)\n",
    "# my_training(batch, epochs, no_iters, lr, bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdLs97J7gxv2"
   },
   "source": [
    "# Separate Models for different Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46x2XExmDPzG"
   },
   "outputs": [],
   "source": [
    "def learn(batch, epochs, no_iters, lr, bi, train_summary_writer, comment):\n",
    "\n",
    "    mean_loss_11 = tf.keras.metrics.Mean()\n",
    "    mean_loss_22 = tf.keras.metrics.Mean()\n",
    "    mean_loss_12 = tf.keras.metrics.Mean()\n",
    "    mean_loss_21 = tf.keras.metrics.Mean()\n",
    "\n",
    "    metrics = [tf.keras.metrics.mean_absolute_error]\n",
    "    model_domain = MyModel(bi=bi)\n",
    "    model_coil = MyModel(bi=bi)\n",
    "\n",
    "    optimizer_domain = tf.keras.optimizers.Nadam(clipvalue = 1, learning_rate= lr)\n",
    "    optimizer_coil = tf.keras.optimizers.Nadam(clipvalue = 1, learning_rate= lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        for i in range(no_iters):\n",
    "\n",
    "            index = np.random.randint(batch.data_size-batch.bs)\n",
    "\n",
    "            # X_domain, X_coil, y_domain, y_coil, mask_domain, mask_coil, seq_len_domain + seq_len_interface, seq_len_coil + seq_len_interface\n",
    "\n",
    "\n",
    "            X, Y, masks, src_len = batch.data_feed(index)\n",
    "            Y = [np.expand_dims(y*1E0, axis=-1) for y in Y]\n",
    "            # Y_domain, Y_coil, Y_interface = Y\n",
    "            Y_domain, Y_coil = Y\n",
    "\n",
    "            masks = [np.logical_not(mask) for mask in masks]\n",
    "            masks = [np.logical_not(np.all(mask, axis=-1)) for mask in masks]\n",
    "\n",
    "            # X_domain, X_coil, X_interface = [[x, mask] for x, mask in zip(X, masks)]\n",
    "            X_domain, X_coil = [[x, mask] for x, mask in zip(X, masks)]\n",
    "\n",
    "            with tf.GradientTape() as tape1 :\n",
    "                y_pred_domain = model_domain(X_domain, training=True)\n",
    "                # y_pred_1_int = model_domain(X_interface, training=True)\n",
    "\n",
    "                loss_domain = tf.keras.losses.mean_squared_error(Y_domain, y_pred_domain)\n",
    "                # loss_1_int = tf.keras.losses.mean_squared_error(Y_interface, y_pred_1_int)\n",
    "\n",
    "            # grads11 = tape1.gradient(loss_domain, model_domain.trainable_variables)\n",
    "            # grads12 = tape1.gradient(loss_1_int, model_domain.trainable_variables)\n",
    "\n",
    "            # optimizer_domain.apply_gradients(zip(grads11, model_domain.trainable_variables))\n",
    "            # optimizer_domain.apply_gradients(zip(grads12, model_domain.trainable_variables))\n",
    "\n",
    "            # grads1 = tape1.gradient([loss_domain, loss_1_int], model_domain.trainable_variables)\n",
    "            grads1 = tape1.gradient(loss_domain, model_domain.trainable_variables)\n",
    "            optimizer_domain.apply_gradients(zip(grads1, model_domain.trainable_variables))\n",
    "\n",
    "            with tf.GradientTape() as tape2 :\n",
    "                y_pred_coil = model_coil(X_coil, training=True)\n",
    "                # y_pred_2_int = model_coil(X_interface, training=True)\n",
    "\n",
    "                loss_coil = tf.keras.losses.mean_squared_error(Y_coil, y_pred_coil)\n",
    "                # loss_2_int = tf.keras.losses.mean_squared_error(Y_interface, y_pred_2_int)\n",
    "\n",
    "            grads22 = tape2.gradient(loss_coil, model_coil.trainable_variables)\n",
    "            # grads21 = tape2.gradient(loss_2_int, model_coil.trainable_variables)\n",
    "\n",
    "            optimizer_coil.apply_gradients(zip(grads22, model_coil.trainable_variables))\n",
    "            # optimizer_coil.apply_gradients(zip(grads21, model_coil.trainable_variables))\n",
    "\n",
    "            mean_loss_11(loss_domain)\n",
    "            mean_loss_22(loss_coil)\n",
    "            # mean_loss_12(loss_1_int)\n",
    "            # mean_loss_21(loss_2_int)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        print('Epoch: {:02} | Time: {}m {}s\\t'.format(epoch+1, epoch_mins, epoch_secs))\n",
    "        try:\n",
    "            # print(f'step {epoch+1}, \\\n",
    "            # mean loss 11 = {mean_loss_11.result().numpy():,.2f}, \\\n",
    "            # mean loss 12 = {mean_loss_12.result().numpy():,.2f}, \\\n",
    "            # mean loss 22 = {mean_loss_22.result().numpy():,.2f},\\\n",
    "            # mean loss 21 = {mean_loss_21.result().numpy():,.2f}')\n",
    "\n",
    "            print(f'step {epoch+1}, \\\n",
    "            mean loss 11 = {mean_loss_11.result().numpy():,.2f}, \\\n",
    "            mean loss 22 = {mean_loss_22.result().numpy():,.2f}')\n",
    "        except:\n",
    "            # print(f'step {epoch+1}, \\\n",
    "            # mean loss 11 = {mean_loss_11.result():,.2f} \\\n",
    "            # mean loss 11 = {mean_loss_12.result():,.2f} \\\n",
    "            # mean loss 11 = {mean_loss_22.result():,.2f} \\\n",
    "            # mean loss 11 = {mean_loss_21.result():,.2f}')\n",
    "\n",
    "            print(f'step {epoch+1}, \\\n",
    "            mean loss 11 = {mean_loss_11.result():,.2f} \\\n",
    "            mean loss 11 = {mean_loss_22.result():,.2f}')\n",
    "\n",
    "        # with train_summary_writer.as_default():\n",
    "        #     tf.summary.scalar('loss', mean_loss.result(), step=epoch)\n",
    "\n",
    "    #         try:\n",
    "    #             for lay in model.layers:\n",
    "    #                 lay_weights = lay.get_weights()\n",
    "    #                 lay_name = lay.name\n",
    "    #                 for idx, lay_wnb in enumerate(lay_weights):\n",
    "    # #                    print(f'{lay_name}_{idx}', lay_wnb.shape)\n",
    "    #                     tf.summary.histogram(f'{lay_name}_{idx}', lay_wnb, epoch)\n",
    "    #         except:\n",
    "    #             pass\n",
    "\n",
    "        # y_pred = model(X_train, training=False)\n",
    "        # y_pred = y_pred.numpy()\n",
    "\n",
    "        # mean_loss.reset_states()\n",
    "        # tf.summary.flush(train_summary_writer)\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss_domain', mean_loss_11.result(), step=epoch)\n",
    "            tf.summary.scalar('loss_coil', mean_loss_22.result(), step=epoch)\n",
    "\n",
    "        X_d = X_domain[0]\n",
    "        X_c = X_coil[0]\n",
    "        Y_d, Y_c = Y\n",
    "\n",
    "        X_print = np.concatenate((X_d[0], X_c[0]), axis=0)\n",
    "        y_print = np.concatenate((Y_d[0], Y_c[0]), axis=0)\n",
    "\n",
    "#         print(\"y_print============================\")\n",
    "#         print(y_print)\n",
    "\n",
    "        y_pred_domain = model_domain(X_domain, training=False)\n",
    "        y_pred_domain = y_pred_domain.numpy()\n",
    "\n",
    "        y_pred_coil = model_coil(X_coil, training=False)\n",
    "        y_pred_coil = y_pred_coil.numpy()\n",
    "\n",
    "#         print(\"y_pred_domain====================\")\n",
    "#         print(y_pred_domain)\n",
    "\n",
    "        y_pred = np.concatenate((y_pred_domain[0], y_pred_coil[0]), axis=0)\n",
    "\n",
    "#         print(\"y_pred===========================\")\n",
    "#         print(y_pred)\n",
    "\n",
    "        #print('y_pred.shape', y_pred.shape)\n",
    "        batch.save_plots(X_print[:, 0], X_print[:, 1], y_print[:,0], y_pred[:, 0], epoch, comment)\n",
    "\n",
    "        mean_loss_11.reset_states()\n",
    "        mean_loss_22.reset_states()\n",
    "        tf.summary.flush(train_summary_writer)\n",
    "\n",
    "    # Serializing json\n",
    "    #history_object = json.dumps(history, indent = 4)\n",
    "    batch.save_stats(history, epoch, comment)\n",
    "    #print(history_object)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKiSJjR1128b",
    "outputId": "401c9a44-1220-42e8-9294-2e900283b9c4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer my_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer my_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch: 01 | Time: 41m 59s\t\n",
      "step 1,             mean loss 11 = 0.12,             mean loss 22 = 0.12\n",
      "Epoch: 02 | Time: 41m 54s\t\n",
      "step 2,             mean loss 11 = 0.04,             mean loss 22 = 0.05\n",
      "Epoch: 03 | Time: 41m 54s\t\n",
      "step 3,             mean loss 11 = 0.03,             mean loss 22 = 0.04\n"
     ]
    }
   ],
   "source": [
    "# learn(batch, epochs, no_iters, lr, bi)\n",
    "history = learn(batch, epochs, no_iters, lr, bi, train_summary_writer, comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2BdRr74r128e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1sXtZVIS2dE_vxPB4xqEi3G3GIYp_uS19",
     "timestamp": 1664846502605
    },
    {
     "file_id": "1-_SwqymVYxOE_C4aWLVBnqJraJwEcFfS",
     "timestamp": 1664241316591
    },
    {
     "file_id": "18pCmZQLI3HmXhUYoEldiJOzbWC7jInpV",
     "timestamp": 1662931024304
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
